#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç©¶æ¥µã®PCç’°å¢ƒãƒ»GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«

ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã€Pythonç’°å¢ƒã®æ•´åˆæ€§ã€GPUæ€§èƒ½ã€ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å‹•ä½œã‚’
ä¸€ã¤ã®GUIã§è¨ºæ–­ãƒ»åˆ†æã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± (OS, CPU, ãƒ¡ãƒ¢ãƒª)
- GPUè©³ç´°æƒ…å ± (nvidia-smiã‚’ä½¿ç”¨)
- PyTorchã¨CUDAã®é€£æºãƒ†ã‚¹ãƒˆ
- CPU vs GPUã®è¡Œåˆ—è¨ˆç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (100x100, 10000x10000)
- VSCodeã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®Pythonç’°å¢ƒæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
- ä¸»è¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
pip install FreeSimpleGUI torch psutil cupy-cuda12x nvidia-ml-py3
(cupy-cuda12xã®éƒ¨åˆ†ã¯ã”è‡ªèº«ã®CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆã‚ã›ã¦ãã ã•ã„)
"""
import sys
import os
import platform
import subprocess
import time
import importlib
import json

# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨GUIåˆ©ç”¨å¯å¦ã®åˆ¤å®š ---
try:
    import FreeSimpleGUI as sg
    GUI_AVAILABLE = True
except ImportError:
    GUI_AVAILABLE = False
    print("è­¦å‘Š: FreeSimpleGUIãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GUIã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰: pip install FreeSimpleGUI")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# --- å„ç¨®è¨ºæ–­æ©Ÿèƒ½ ---

def get_system_info():
    """OSã€CPUã€ãƒ¡ãƒ¢ãƒªãªã©ã®åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ---"]
    if not PSUTIL_AVAILABLE:
        report.append("âŒ psutilãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©³ç´°ãªã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚")
        report.append("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰: pip install psutil")
        return report

    try:
        report.append(f"OS: {platform.system()} {platform.release()} ({platform.machine()})")
        report.append(f"ãƒ—ãƒ­ã‚»ãƒƒã‚µ: {platform.processor()}")
        
        # CPUæƒ…å ±
        cpu_cores_physical = psutil.cpu_count(logical=False)
        cpu_cores_logical = psutil.cpu_count(logical=True)
        report.append(f"CPUã‚³ã‚¢æ•°: {cpu_cores_physical} (ç‰©ç†) / {cpu_cores_logical} (è«–ç†)")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        total_mem_gb = memory.total / (1024**3)
        available_mem_gb = memory.available / (1024**3)
        report.append(f"ãƒ¡ãƒ¢ãƒª(RAM): åˆè¨ˆ {total_mem_gb:.2f} GB / ç©ºã {available_mem_gb:.2f} GB ({memory.percent} % ä½¿ç”¨ä¸­)")

    except Exception as e:
        report.append(f"âŒ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return report

def get_gpu_info():
    """nvidia-smiã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦GPUã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸ® GPUæƒ…å ± (nvidia-smi) ---"]
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,driver_version,memory.total,memory.used,utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, check=True, encoding='utf-8'
        )
        gpus = result.stdout.strip().split('\n')
        report.append(f"âœ… NVIDIA GPUã‚’ {len(gpus)} å°æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        for i, line in enumerate(gpus):
            name, driver, mem_total, mem_used, util, temp = [p.strip() for p in line.split(',')]
            report.append(f"  [GPU {i}]")
            report.append(f"    ãƒ¢ãƒ‡ãƒ«å: {name}")
            report.append(f"    ãƒ‰ãƒ©ã‚¤ãƒVer: {driver}")
            report.append(f"    ãƒ¡ãƒ¢ãƒª: {mem_used} MB / {mem_total} MB ä½¿ç”¨ä¸­")
            report.append(f"    ä½¿ç”¨ç‡: {util} %")
            report.append(f"    æ¸©åº¦: {temp} Â°C")

    except FileNotFoundError:
        report.append("âš ï¸ nvidia-smiã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚NVIDIAãƒ‰ãƒ©ã‚¤ãƒãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    except subprocess.CalledProcessError as e:
        report.append(f"âŒ nvidia-smiã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e.stderr}")
    except Exception as e:
        report.append(f"âŒ GPUæƒ…å ±ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return report

def get_pytorch_info():
    """PyTorchã¨CUDAã®é€£æºçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸ”¥ PyTorch & CUDAé€£æº ---"]
    if not TORCH_AVAILABLE:
        report.append("âŒ PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        report.append("   å…¬å¼ã‚µã‚¤ãƒˆã‚’å‚è€ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: https://pytorch.org/")
        return report

    report.append(f"âœ… PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    
    cuda_available = torch.cuda.is_available()
    report.append(f"CUDAåˆ©ç”¨å¯èƒ½ã‹: {'âœ… ã¯ã„' if cuda_available else 'âŒ ã„ã„ãˆ'}")

    if cuda_available:
        report.append(f"PyTorchãƒ“ãƒ«ãƒ‰æ™‚CUDA Ver: {torch.version.cuda}")
        device_count = torch.cuda.device_count()
        report.append(f"æ¤œå‡ºã•ã‚ŒãŸCUDAãƒ‡ãƒã‚¤ã‚¹æ•°: {device_count}")
        for i in range(device_count):
            device_name = torch.cuda.get_device_name(i)
            total_mem_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            report.append(f"  GPU {i}: {device_name} ({total_mem_gb:.2f} GB)")
        
        cudnn_ver = torch.backends.cudnn.version()
        report.append(f"cuDNN ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cudnn_ver}")
    else:
        report.append("âš ï¸ GPUã¯ã‚·ã‚¹ãƒ†ãƒ ã«å­˜åœ¨ã—ã¾ã™ãŒã€PyTorchãŒCUDAã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        report.append("   PyTorchãŒCPUç‰ˆã¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        report.append("   GPUå¯¾å¿œç‰ˆã®PyTorchã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        
    return report

def check_environment_consistency():
    """VSCodeã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®Pythonç’°å¢ƒã®æ•´åˆæ€§ã‚’è¨ºæ–­ã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸ Pythonç’°å¢ƒ æ•´åˆæ€§è¨ºæ–­ ---"]
    try:
        # 1. ç¾åœ¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒä½¿ç”¨ã—ã¦ã„ã‚‹Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿
        script_interpreter = sys.executable
        report.append(f"ğŸ“œ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­ã®Python: {script_interpreter}")

        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ä»®æƒ³ç’°å¢ƒãƒ‘ã‚¹ã‚’å–å¾—
        venv_path = os.environ.get('VIRTUAL_ENV')
        conda_env_path = os.environ.get('CONDA_PREFIX')
        report.append(f"ğŸ“¦ VIRTUAL_ENV (venv/virtualenv): {venv_path or 'æœªè¨­å®š'}")
        report.append(f"ğŸ“¦ CONDA_PREFIX (Conda): {conda_env_path or 'æœªè¨­å®š'}")

        # 3. ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãŒèªè­˜ã™ã‚‹ `python` ã‚³ãƒãƒ³ãƒ‰ã®ãƒ‘ã‚¹
        shell_command = 'where' if platform.system() == "Windows" else 'which'
        result = subprocess.run([shell_command, 'python'], capture_output=True, text=True, encoding='utf-8')
        terminal_interpreter = result.stdout.strip().split('\n')[0]
        report.append(f"ğŸ–¥ï¸ ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®'python'ã‚³ãƒãƒ³ãƒ‰: {terminal_interpreter}")
        
        # --- è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ ---
        report.append("\n[è¨ºæ–­çµæœ]")
        match = True
        
        # VSCode(ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)ã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹
        if os.path.normcase(script_interpreter) != os.path.normcase(terminal_interpreter):
            report.append("âš ï¸ ä¸ä¸€è‡´: ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­ã®Pythonã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®PythonãŒç•°ãªã‚Šã¾ã™ã€‚")
            match = False

        # ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹ãªã®ã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãã®ä¸­ã®Pythonã‚’ä½¿ã£ã¦ã„ãªã„å ´åˆ
        active_env_path = venv_path or conda_env_path
        if active_env_path and not script_interpreter.startswith(active_env_path):
            report.append("âš ï¸ ä¸ä¸€è‡´: ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ãŒã€VSCodeãŒåˆ¥ã®Pythonã‚’è¦‹ã¦ã„ã¾ã™ã€‚")
            report.append(f"   (æœ‰åŠ¹ãªç’°å¢ƒ: {active_env_path})")
            match = False
            
        if match:
            report.append("âœ… è‰¯å¥½: VSCodeã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã¯åŒã˜Pythonç’°å¢ƒã‚’èªè­˜ã—ã¦ã„ã¾ã™ã€‚")
        else:
            report.append("\n[å¯¾ç­–]")
            report.append("  - VSCodeã®å³ä¸‹ã§Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’å†é¸æŠã—ã¦ãã ã•ã„ã€‚")
            report.append(f"  - ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ '{active_env_path}' ãŒæ­£ã—ãæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    except Exception as e:
        report.append(f"âŒ æ•´åˆæ€§è¨ºæ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return report

def run_performance_benchmark():
    """CPUã¨GPUã®è¡Œåˆ—è¨ˆç®—é€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸš€ CPU vs GPU ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (è¡Œåˆ—è¨ˆç®—) ---"]
    if not TORCH_AVAILABLE:
        report.append("âŒ PyTorchãŒãªã„ãŸã‚ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
        return report

    def measure_time(device, size):
        try:
            tensor_a = torch.randn(size, size, device=device)
            tensor_b = torch.randn(size, size, device=device)
            
            if device == 'cuda':
                torch.cuda.synchronize()
            
            start_time = time.perf_counter()
            for _ in range(5):  # è¤‡æ•°å›å®Ÿè¡Œã—ã¦å¹³å‡åŒ–
                torch.matmul(tensor_a, tensor_b)
            
            if device == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.perf_counter()
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del tensor_a, tensor_b
            if device == 'cuda':
                torch.cuda.empty_cache()

            return (end_time - start_time) / 5
        except torch.cuda.OutOfMemoryError:
            return "ãƒ¡ãƒ¢ãƒªä¸è¶³"
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}"

    # --- 100x100 (è»½é‡) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ---
    report.append("\n[è»½é‡ãƒ†ã‚¹ãƒˆ: 100x100 è¡Œåˆ—]")
    cpu_time_light = measure_time('cpu', 100)
    report.append(f"  CPU å®Ÿè¡Œæ™‚é–“: {cpu_time_light * 1000:.4f} ms")

    if torch.cuda.is_available():
        gpu_time_light = measure_time('cuda', 100)
        report.append(f"  GPU å®Ÿè¡Œæ™‚é–“: {gpu_time_light * 1000:.4f} ms")
        if isinstance(cpu_time_light, float) and isinstance(gpu_time_light, float) and gpu_time_light > 0:
            speedup = cpu_time_light / gpu_time_light
            report.append(f"  ğŸš€ é€Ÿåº¦æ¯”: GPUã¯CPUã® {speedup:.2f} å€é«˜é€Ÿ")
    
    # --- 10000x10000 (é‡é‡) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ---
    report.append("\n[é‡é‡ãƒ†ã‚¹ãƒˆ: 10000x10000 è¡Œåˆ—]")
    cpu_time_heavy = measure_time('cpu', 10000)
    if isinstance(cpu_time_heavy, float):
        report.append(f"  CPU å®Ÿè¡Œæ™‚é–“: {cpu_time_heavy:.4f} ç§’")
    else:
        report.append(f"  CPU å®Ÿè¡Œçµæœ: {cpu_time_heavy}")


    if torch.cuda.is_available():
        gpu_time_heavy = measure_time('cuda', 10000)
        if isinstance(gpu_time_heavy, float):
            report.append(f"  GPU å®Ÿè¡Œæ™‚é–“: {gpu_time_heavy:.4f} ç§’")
            if isinstance(cpu_time_heavy, float) and gpu_time_heavy > 0:
                speedup = cpu_time_heavy / gpu_time_heavy
                report.append(f"  ğŸš€ğŸš€ é€Ÿåº¦æ¯”: GPUã¯CPUã® {speedup:.2f} å€é«˜é€Ÿ (å¤§è¦æ¨¡è¨ˆç®—)")
        else:
            report.append(f"  GPU å®Ÿè¡Œçµæœ: {gpu_time_heavy}")
            
    return report

def check_key_packages():
    """ä¸»è¦ãªæ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚"""
    report = ["\n--- ğŸ“š ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª ---"]
    packages = [
        'numpy', 'pandas', 'matplotlib', 'scipy', 
        'sklearn', 'cupy', 'psutil', 'requests'
    ]
    
    for package in packages:
        try:
            module = importlib.import_module(package)
            version = getattr(module, '__version__', 'ä¸æ˜')
            report.append(f"  âœ… {package.ljust(12)}: v{version}")
        except ImportError:
            report.append(f"  âŒ {package.ljust(12)}: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        except Exception as e:
            report.append(f"  âš ï¸ {package.ljust(12)}: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ ({e})")
            
    return report

def run_all_diagnostics():
    """å…¨ã¦ã®è¨ºæ–­æ©Ÿèƒ½ã‚’é †ç•ªã«å®Ÿè¡Œã—ã€çµæœã‚’ä¸€ã¤ã®æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã¾ã™ã€‚"""
    full_report = []
    full_report.append("ç©¶æ¥µã®PCç’°å¢ƒãƒ»GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ« - è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ")
    full_report.append(f"è¨ºæ–­æ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å„è¨ºæ–­ã‚’å®Ÿè¡Œ
    full_report.extend(get_system_info())
    full_report.extend(get_gpu_info())
    full_report.extend(get_pytorch_info())
    full_report.extend(check_environment_consistency())
    full_report.extend(check_key_packages())
    full_report.extend(run_performance_benchmark())
    
    full_report.append("\n--- âœ… è¨ºæ–­å®Œäº† ---")
    
    return "\n".join(full_report)

# --- GUIãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main_gui():
    """FreeSimpleGUIã‚’ä½¿ç”¨ã—ã¦GUIã‚’æ§‹ç¯‰ãƒ»å®Ÿè¡Œã—ã¾ã™ã€‚"""
    sg.theme("SystemDefault")

    layout = [
        [sg.Text("ç©¶æ¥µã®PCç’°å¢ƒãƒ»GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«", font=("Meiryo", 16, "bold"))],
        [sg.Multiline(
            "ã€Œè§£æå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚",
            size=(100, 30),
            key="-OUTPUT-",
            font=("Consolas", 10),
            disabled=True
        )],
        [
            sg.Button("è§£æå®Ÿè¡Œ", size=(12, 1), key="-RUN-"),
            sg.Button("ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜", size=(12, 1), key="-SAVE-", disabled=True),
            sg.Button("çµ‚äº†", size=(10, 1), key="-EXIT-")
        ],
        [sg.StatusBar("", size=(80, 1), key="-STATUS-")]
    ]

    window = sg.Window("ç©¶æ¥µã®ç’°å¢ƒã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼", layout, finalize=True)
    
    report_content = ""

    while True:
        event, values = window.read()

        if event in (sg.WINDOW_CLOSED, "-EXIT-"):
            break
        
        elif event == "-RUN-":
            window["-RUN-"].update(disabled=True)
            window["-SAVE-"].update(disabled=True)
            window["-STATUS-"].update("è§£æã‚’å®Ÿè¡Œä¸­... ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
            window["-OUTPUT-"].update("è§£æã‚’å®Ÿè¡Œä¸­...\n\nCPUã¨GPUã®é‡é‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆ10000x10000è¡Œåˆ—è¨ˆç®—ï¼‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã®ã§ã€ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
            window.refresh() # UIã‚’å³æ™‚æ›´æ–°

            report_content = run_all_diagnostics()
            
            window["-OUTPUT-"].update(report_content)
            window["-STATUS-"].update("è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            window["-RUN-"].update(disabled=False)
            window["-SAVE-"].update(disabled=False)

        elif event == "-SAVE-":
            try:
                save_path = sg.popup_get_file(
                    "ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜",
                    save_as=True,
                    default_extension=".txt",
                    file_types=(("Text Files", "*.txt"), ("All Files", "*.*")),
                    no_window=True
                )
                if save_path:
                    with open(save_path, 'w', encoding='utf-8') as f:
                        f.write(report_content)
                    sg.popup("æˆåŠŸ", f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ {save_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                sg.popup_error("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    window.close()

if __name__ == "__main__":
    if GUI_AVAILABLE:
        main_gui()
    else:
        # GUIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("--- CUIãƒ¢ãƒ¼ãƒ‰ã§è¨ºæ–­ã‚’å®Ÿè¡Œã—ã¾ã™ ---")
        report = run_all_diagnostics()
        print(report)